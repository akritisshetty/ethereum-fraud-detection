{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ethereum Fraud Detection Using Graph Neural Networks (GNN)"
      ],
      "metadata": {
        "id": "1ULLjtDv_vvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "z0eccSdz_RYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25206f7c-1d48-412d-ffa7-f85631080882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, warnings, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from datetime import datetime\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from imblearn.combine import SMOTEENN\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "MODEL_DIR = \"models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Load data\n",
        "DATA_PATH = \"transaction_dataset.csv\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "LABEL_CANDIDATES = ['FLAG', 'label', 'Label', 'fraud', 'isFraud']\n",
        "LABEL_COL = next((c for c in LABEL_CANDIDATES if c in df.columns), df.columns[-1])\n",
        "df[LABEL_COL] = df[LABEL_COL].astype(int)\n",
        "\n",
        "# Preprocessing\n",
        "id_cols = ['Address', 'Unnamed: 0', 'Index']\n",
        "df.drop(columns=[c for c in id_cols if c in df.columns], inplace=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "y = df[LABEL_COL].copy()\n",
        "X = df.drop(columns=[LABEL_COL]).copy()\n",
        "\n",
        "for c in X.select_dtypes(include=['object','category']).columns:\n",
        "    X[c], _ = pd.factorize(X[c])\n",
        "X = X.loc[:, X.nunique() > 1]\n",
        "\n",
        "# Train/Validation/Test Split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=SEED)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.17647, stratify=y_temp, random_state=SEED)\n",
        "\n",
        "# Feature selection\n",
        "K = min(50, X_train.shape[1])\n",
        "selector = SelectKBest(score_func=mutual_info_classif, k=K)\n",
        "selector.fit(X_train, y_train)\n",
        "selected_features = X_train.columns[selector.get_support()].tolist()\n",
        "\n",
        "corr_matrix = X_train[selected_features].corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "high_corr = [c for c in upper.columns if any(upper[c] > 0.9)]\n",
        "for c in high_corr:\n",
        "    selected_features.remove(c)\n",
        "\n",
        "X_train_sel = X_train[selected_features]\n",
        "X_val_sel = X_val[selected_features]\n",
        "X_test_sel = X_test[selected_features]\n",
        "joblib.dump(selected_features, os.path.join(MODEL_DIR, \"selected_features.pkl\"))\n",
        "\n",
        "# Scaling\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_sel), columns=selected_features)\n",
        "X_val_scaled = pd.DataFrame(scaler.transform(X_val_sel), columns=selected_features)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test_sel), columns=selected_features)\n",
        "joblib.dump(scaler, os.path.join(MODEL_DIR, \"scaler.pkl\"))\n",
        "\n",
        "# Handle class imbalance\n",
        "smote_enn = SMOTEENN(random_state=SEED)\n",
        "X_train_bal_np, y_train_bal = smote_enn.fit_resample(X_train_scaled, y_train)\n",
        "X_train_bal = pd.DataFrame(X_train_bal_np, columns=selected_features)\n",
        "\n",
        "# Base models\n",
        "xgb_params = dict(n_estimators=500, max_depth=10, learning_rate=0.05,\n",
        "                  subsample=0.8, colsample_bytree=0.8, random_state=SEED,\n",
        "                  use_label_encoder=False, eval_metric='logloss')\n",
        "lgb_params = dict(n_estimators=500, learning_rate=0.05, random_state=SEED)\n",
        "\n",
        "base_models = {\n",
        "    'XGB': xgb.XGBClassifier(**xgb_params),\n",
        "    'LGB': lgb.LGBMClassifier(**lgb_params),\n",
        "    'RF': RandomForestClassifier(n_estimators=400, max_depth=20, class_weight='balanced', random_state=SEED, n_jobs=-1),\n",
        "    'GB': GradientBoostingClassifier(n_estimators=300, max_depth=8, learning_rate=0.05, random_state=SEED)\n",
        "}\n",
        "\n",
        "fitted_models = {name: clf.fit(X_train_bal, y_train_bal) for name, clf in base_models.items()}\n",
        "\n",
        "# Evaluate base models\n",
        "results = {}\n",
        "for name, clf in fitted_models.items():\n",
        "    y_val_pred = clf.predict(X_val_scaled)\n",
        "    y_val_prob = clf.predict_proba(X_val_scaled)[:,1]\n",
        "    results[name] = {\n",
        "        'accuracy': accuracy_score(y_val, y_val_pred),\n",
        "        'precision': precision_score(y_val, y_val_pred, zero_division=0),\n",
        "        'recall': recall_score(y_val, y_val_pred, zero_division=0),\n",
        "        'f1': f1_score(y_val, y_val_pred, zero_division=0),\n",
        "        'auc': roc_auc_score(y_val, y_val_prob)\n",
        "    }\n",
        "\n",
        "best_base_name = max(results, key=lambda n: results[n]['f1'])\n",
        "best_base_model = fitted_models[best_base_name]\n",
        "\n",
        "# Stacking ensemble\n",
        "top_models = sorted(results.items(), key=lambda x: x[1]['f1'], reverse=True)[:2]\n",
        "estimators = [(name, fitted_models[name]) for name,_ in top_models]\n",
        "\n",
        "stack_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=1000, class_weight='balanced', random_state=SEED),\n",
        "    cv=5, n_jobs=-1, passthrough=False\n",
        ")\n",
        "stack_clf.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "y_stack_val_pred = stack_clf.predict(X_val_scaled)\n",
        "stack_f1 = f1_score(y_val, y_stack_val_pred)\n",
        "final_model = stack_clf if stack_f1 > results[best_base_name]['f1'] else best_base_model\n",
        "final_model_name = 'Stacking' if final_model==stack_clf else best_base_name\n",
        "joblib.dump(final_model, os.path.join(MODEL_DIR, \"final_model.pkl\"))\n",
        "\n",
        "# Optimize threshold\n",
        "y_val_prob = final_model.predict_proba(X_val_scaled)[:,1]\n",
        "thresholds = np.arange(0.3,0.7,0.01)\n",
        "best_f1_thresh, best_thresh = 0, 0.5\n",
        "for t in thresholds:\n",
        "    preds = (y_val_prob>=t).astype(int)\n",
        "    f1 = f1_score(y_val, preds)\n",
        "    if f1 > best_f1_thresh:\n",
        "        best_f1_thresh, best_thresh = f1, t\n",
        "print(f\"Optimal threshold based on val F1: {best_thresh:.2f}\")\n",
        "\n",
        "# Test evaluation\n",
        "y_test_prob = final_model.predict_proba(X_test_scaled)[:,1]\n",
        "y_test_pred = (y_test_prob>=best_thresh).astype(int)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "test_prec = precision_score(y_test, y_test_pred)\n",
        "test_rec = recall_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred)\n",
        "test_auc = roc_auc_score(y_test, y_test_prob)\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Test Acc: {test_acc:.4f} | Prec: {test_prec:.4f} | Rec: {test_rec:.4f} | F1: {test_f1:.4f} | AUC: {test_auc:.4f}\")\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, y_test_pred, target_names=['Clean','Fraud']))\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'final_model_name': final_model_name,\n",
        "    'optimal_threshold': float(best_thresh),\n",
        "    'test_accuracy': float(test_acc),\n",
        "    'test_precision': float(test_prec),\n",
        "    'test_recall': float(test_rec),\n",
        "    'test_f1': float(test_f1),\n",
        "    'test_auc': float(test_auc),\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "joblib.dump(metadata, os.path.join(MODEL_DIR, \"metadata.pkl\"))\n",
        "\n",
        "# Inference function\n",
        "def predict_fraud(transaction_data):\n",
        "    model = joblib.load(os.path.join(MODEL_DIR, \"final_model.pkl\"))\n",
        "    scaler = joblib.load(os.path.join(MODEL_DIR, \"scaler.pkl\"))\n",
        "    features = joblib.load(os.path.join(MODEL_DIR, \"selected_features.pkl\"))\n",
        "    metadata = joblib.load(os.path.join(MODEL_DIR, \"metadata.pkl\"))\n",
        "    threshold = metadata['optimal_threshold']\n",
        "\n",
        "    if isinstance(transaction_data, dict):\n",
        "        transaction_data = pd.DataFrame([transaction_data])\n",
        "    if not isinstance(transaction_data, pd.DataFrame):\n",
        "        raise ValueError(\"Input must be DataFrame or dict\")\n",
        "\n",
        "    for f in features:\n",
        "        if f not in transaction_data.columns:\n",
        "            transaction_data[f] = 0\n",
        "\n",
        "    X_new = transaction_data[features].copy()\n",
        "    X_new_scaled = scaler.transform(X_new)\n",
        "    probs = model.predict_proba(X_new_scaled)[:,1]\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "    risk_levels = ['Very Low' if p<0.25 else 'Low' if p<0.5 else 'Medium' if p<0.75 else 'High' for p in probs]\n",
        "\n",
        "    return {'predictions': preds, 'fraud_probability': probs, 'risk_level': risk_levels}"
      ],
      "metadata": {
        "id": "InA7CIzn_q3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8624ee40-4383-45f6-f809-da79103ddb49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal threshold based on val F1: 0.30\n",
            "Test Acc: 0.9971 | Prec: 1.0000 | Rec: 0.9819 | F1: 0.9909 | AUC: 0.9985\n",
            "Confusion matrix:\n",
            " [[863   0]\n",
            " [  3 163]]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       Clean       1.00      1.00      1.00       863\n",
            "       Fraud       1.00      0.98      0.99       166\n",
            "\n",
            "    accuracy                           1.00      1029\n",
            "   macro avg       1.00      0.99      0.99      1029\n",
            "weighted avg       1.00      1.00      1.00      1029\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(100)\n",
        "n_clean, n_fraud = 100, 100\n",
        "top_features = selected_features[:5]\n",
        "\n",
        "synthetic_data = {}\n",
        "fraud_stats = X_train[y_train==1][selected_features] if sum(y_train==1) > 0 else None\n",
        "\n",
        "for feature in selected_features:\n",
        "    mean_val = X_train[feature].mean()\n",
        "    std_val = X_train[feature].std()\n",
        "    clean_samples = np.random.normal(mean_val, std_val, n_clean)\n",
        "\n",
        "    if fraud_stats is not None and feature in fraud_stats.columns:\n",
        "        fraud_mean = fraud_stats[feature].mean()\n",
        "        fraud_std = fraud_stats[feature].std()\n",
        "    else:\n",
        "        fraud_mean, fraud_std = mean_val*1.5, std_val*2\n",
        "\n",
        "    if feature in top_features:\n",
        "        fraud_samples = np.random.normal(fraud_mean*1.5, fraud_std*1.5, n_fraud)\n",
        "    else:\n",
        "        fraud_samples = np.random.normal(fraud_mean*1.2, fraud_std*1.2, n_fraud)\n",
        "\n",
        "    synthetic_data[feature] = np.concatenate([clean_samples, fraud_samples])\n",
        "\n",
        "synthetic_df = pd.DataFrame(synthetic_data)\n",
        "\n",
        "def add_noise(df, noise_level=0.05):\n",
        "    noise = np.random.normal(0, noise_level, df.shape)\n",
        "    stds = df.std().to_numpy().reshape(1, -1)\n",
        "    return df + noise * stds\n",
        "\n",
        "synthetic_df.iloc[:n_clean] = add_noise(synthetic_df.iloc[:n_clean], 0.03)\n",
        "synthetic_df.iloc[n_clean:] = add_noise(synthetic_df.iloc[n_clean:], 0.1)\n",
        "\n",
        "synthetic_labels = np.array([0]*n_clean + [1]*n_fraud)\n",
        "synthetic_df['isFraud'] = synthetic_labels\n",
        "\n",
        "SYNTHETIC_PATH = os.path.join(MODEL_DIR, \"synthetic_test_dataset.csv\")\n",
        "synthetic_df.to_csv(SYNTHETIC_PATH, index=False)\n",
        "print(f\"Synthetic dataset saved to: {SYNTHETIC_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UELZtmrLVNmd",
        "outputId": "c696efb9-f0f3-4388-e6da-1ceb13881f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic dataset saved to: models/synthetic_test_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_synth_scaled = scaler.transform(synthetic_df[selected_features])\n",
        "y_synth_prob = final_model.predict_proba(X_synth_scaled)[:,1]\n",
        "y_synth_pred = (y_synth_prob >= best_thresh).astype(int)\n",
        "\n",
        "n_total = n_clean + n_fraud\n",
        "n_fraud_pred = y_synth_pred.sum()\n",
        "n_clean_pred = n_total - n_fraud_pred\n",
        "synthetic_accuracy = ((y_synth_pred[:n_clean]==0).sum() + (y_synth_pred[n_clean:]==1).sum()) / n_total\n",
        "precision = (y_synth_pred[n_clean:] == 1).sum() / max(y_synth_pred.sum(), 1)\n",
        "recall = (y_synth_pred[n_clean:] == 1).sum() / n_fraud\n",
        "\n",
        "risk_levels = ['Very Low' if p<0.25 else 'Low' if p<0.5 else 'Medium' if p<0.75 else 'High' for p in y_synth_prob]\n",
        "risk_counts = pd.Series(risk_levels).value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"Synthetic Inference Dataset Statistics:\")\n",
        "print(f\"Total Transactions: {n_total}\")\n",
        "print(f\"Fraud Predictions: {n_fraud_pred} ({n_fraud_pred/n_total*100:.1f}%)\")\n",
        "print(f\"Clean Predictions: {n_clean_pred} ({n_clean_pred/n_total*100:.1f}%)\")\n",
        "print(f\"Synthetic Accuracy: {synthetic_accuracy*100:.2f}%\")\n",
        "print(f\"Precision: {precision*100:.2f}%\")\n",
        "print(f\"Recall: {recall*100:.2f}%\")\n",
        "print(\"Risk Breakdown:\")\n",
        "for r, pct in risk_counts.items():\n",
        "    print(f\"  {r:<9}: {pct:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qbbuk-IUVHiN",
        "outputId": "6c535cd2-0478-4168-d69c-08d8896f5884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic Inference Dataset Statistics:\n",
            "Total Transactions: 200\n",
            "Fraud Predictions: 115 (57.5%)\n",
            "Clean Predictions: 85 (42.5%)\n",
            "Synthetic Accuracy: 81.50%\n",
            "Precision: 77.39%\n",
            "Recall: 89.00%\n",
            "Risk Breakdown:\n",
            "  High     : 52.0%\n",
            "  Very Low : 42.5%\n",
            "  Medium   : 4.5%\n",
            "  Low      : 1.0%\n"
          ]
        }
      ]
    }
  ]
}