{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                             precision_recall_curve, auc, f1_score,\n",
        "                             precision_score, recall_score, roc_auc_score)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "1CH48k_Wj9bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ETHEREUM ADDRESS POISONING DETECTION SYSTEM\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wm6_i8Nj_jc",
        "outputId": "afc2ade2-7aa6-4a94-8624-e8e0de516918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ETHEREUM ADDRESS POISONING DETECTION SYSTEM\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PHASE A: FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "class EthereumFeatureEngineering:\n",
        "    \"\"\"\n",
        "    Feature engineering pipeline for Ethereum transaction analysis\n",
        "    Focuses on behavioral patterns indicative of address poisoning attacks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.from_address_history = defaultdict(list)\n",
        "        self.to_address_seen = defaultdict(set)\n",
        "        self.address_freq_from = {}\n",
        "        self.address_freq_to = {}\n",
        "\n",
        "    def load_and_analyze_data(self, filepath):\n",
        "        \"\"\"Load CSV and perform initial analysis\"\"\"\n",
        "        print(\"\\n[1] Loading Dataset...\")\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        print(f\"Dataset Shape: {df.shape}\")\n",
        "        print(f\"\\nColumn Names: {df.columns.tolist()}\")\n",
        "        print(f\"\\nData Types:\\n{df.dtypes}\")\n",
        "        print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
        "        print(f\"\\nClass Distribution:\\n{df['Class'].value_counts()}\")\n",
        "        print(f\"\\nClass Percentage:\\n{df['Class'].value_counts(normalize=True) * 100}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def engineer_features(self, df):\n",
        "        \"\"\"\n",
        "        Transform raw data into behavioral features for fraud detection\n",
        "        \"\"\"\n",
        "        print(\"\\n[2] Engineering Behavioral Features...\")\n",
        "\n",
        "        # Make a copy to avoid modifying original\n",
        "        df = df.copy()\n",
        "\n",
        "        # --- Data Cleaning ---\n",
        "        print(\"   ‚Üí Cleaning data...\")\n",
        "        # Handle missing values in 'To' address (contract creation transactions)\n",
        "        df['To'] = df['To'].fillna('CONTRACT_CREATION')\n",
        "\n",
        "        # Handle missing 'Input' (simple ETH transfers have empty input)\n",
        "        df['Input'] = df['Input'].fillna('0x')\n",
        "\n",
        "        # Convert timestamp to datetime\n",
        "        df['TimeStamp'] = pd.to_datetime(df['TimeStamp'], unit='s')\n",
        "\n",
        "        # Sort by timestamp for temporal features\n",
        "        df = df.sort_values('TimeStamp').reset_index(drop=True)\n",
        "\n",
        "        # --- Feature 1: Value Binning (Dust Detection) ---\n",
        "        print(\"   ‚Üí Creating dust transaction flag...\")\n",
        "        df['is_dust'] = (df['Value'] < 0.001).astype(int)\n",
        "\n",
        "        # Additional value features\n",
        "        df['value_log'] = np.log1p(df['Value'])  # Log transform for skewed distribution\n",
        "        df['is_zero_value'] = (df['Value'] == 0).astype(int)\n",
        "\n",
        "        # --- Feature 2: Address Frequency (Rolling Window) ---\n",
        "        print(\"   ‚Üí Calculating address frequencies...\")\n",
        "\n",
        "        # Calculate transactions per address (frequency encoding)\n",
        "        from_counts = df['From'].value_counts()\n",
        "        to_counts = df['To'].value_counts()\n",
        "\n",
        "        df['from_address_freq'] = df['From'].map(from_counts)\n",
        "        df['to_address_freq'] = df['To'].map(to_counts)\n",
        "\n",
        "        # Store for later use\n",
        "        self.address_freq_from = from_counts.to_dict()\n",
        "        self.address_freq_to = to_counts.to_dict()\n",
        "\n",
        "        # --- Feature 3: Time Delta ---\n",
        "        print(\"   ‚Üí Computing time deltas between transactions...\")\n",
        "\n",
        "        # Calculate time difference from previous transaction (same sender)\n",
        "        df['time_delta_seconds'] = df.groupby('From')['TimeStamp'].diff().dt.total_seconds()\n",
        "        df['time_delta_seconds'] = df['time_delta_seconds'].fillna(0)\n",
        "\n",
        "        # Time-based features\n",
        "        df['hour_of_day'] = df['TimeStamp'].dt.hour\n",
        "        df['day_of_week'] = df['TimeStamp'].dt.dayofweek\n",
        "        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
        "\n",
        "        # --- Feature 4: Address Interaction (New vs Recurring) ---\n",
        "        print(\"   ‚Üí Identifying new vs recurring address interactions...\")\n",
        "\n",
        "        # Track if recipient address is new for this sender\n",
        "        df['is_new_recipient'] = 0\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            from_addr = row['From']\n",
        "            to_addr = row['To']\n",
        "\n",
        "            # Check if this 'To' address has been seen before by this 'From' address\n",
        "            if to_addr not in self.to_address_seen[from_addr]:\n",
        "                df.at[idx, 'is_new_recipient'] = 1\n",
        "                self.to_address_seen[from_addr].add(to_addr)\n",
        "\n",
        "        # --- Feature 5: Transaction Burst Detection ---\n",
        "        print(\"   ‚Üí Detecting transaction bursts...\")\n",
        "\n",
        "        # Count transactions from same address within 1 hour window\n",
        "        df['tx_count_1h'] = df.groupby('From')['TimeStamp'].transform(\n",
        "            lambda x: x.rolling('1H', on=x).count()\n",
        "        )\n",
        "\n",
        "        # --- Feature 6: Contract Interaction Features ---\n",
        "        print(\"   ‚Üí Analyzing contract interactions...\")\n",
        "\n",
        "        df['has_contract_address'] = (df['ContractAddress'].notna()).astype(int)\n",
        "        df['has_input_data'] = (df['Input'] != '0x').astype(int)\n",
        "        df['input_length'] = df['Input'].str.len()\n",
        "\n",
        "        # --- Feature 7: Target Encoding for High Cardinality ---\n",
        "        print(\"   ‚Üí Applying target encoding for addresses...\")\n",
        "\n",
        "        # Target encoding for 'From' addresses\n",
        "        from_target_mean = df.groupby('From')['Class'].mean()\n",
        "        df['from_fraud_rate'] = df['From'].map(from_target_mean)\n",
        "\n",
        "        # Target encoding for 'To' addresses\n",
        "        to_target_mean = df.groupby('To')['Class'].mean()\n",
        "        df['to_fraud_rate'] = df['To'].map(to_target_mean)\n",
        "\n",
        "        # Fill NaN for new addresses\n",
        "        df['from_fraud_rate'] = df['from_fraud_rate'].fillna(df['Class'].mean())\n",
        "        df['to_fraud_rate'] = df['to_fraud_rate'].fillna(df['Class'].mean())\n",
        "\n",
        "        # --- Feature 8: Statistical Features ---\n",
        "        print(\"   ‚Üí Creating statistical aggregations...\")\n",
        "\n",
        "        # Average value sent by this address\n",
        "        from_value_mean = df.groupby('From')['Value'].transform('mean')\n",
        "        df['from_avg_value'] = from_value_mean\n",
        "\n",
        "        # Deviation from average\n",
        "        df['value_deviation'] = abs(df['Value'] - df['from_avg_value'])\n",
        "\n",
        "        print(f\"\\n   ‚úì Feature engineering complete! Total features: {df.shape[1]}\")\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "M3AOuN4QkEO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PHASE B: PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessing pipeline including scaling and imbalance handling\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def prepare_features(self, df):\n",
        "        \"\"\"\n",
        "        Select and prepare features for modeling\n",
        "        \"\"\"\n",
        "        print(\"\\n[3] Preparing Features for Modeling...\")\n",
        "\n",
        "        # Define feature columns (exclude non-numeric and target)\n",
        "        exclude_cols = ['TxHash', 'BlockHeight', 'TimeStamp', 'From', 'To',\n",
        "                       'ContractAddress', 'Input', 'Class']\n",
        "\n",
        "        self.feature_columns = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "        print(f\"   Selected Features ({len(self.feature_columns)}):\")\n",
        "        for feat in self.feature_columns:\n",
        "            print(f\"      - {feat}\")\n",
        "\n",
        "        X = df[self.feature_columns]\n",
        "        y = df['Class']\n",
        "\n",
        "        # Handle any remaining NaN values\n",
        "        X = X.fillna(0)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def split_and_scale(self, X, y):\n",
        "        \"\"\"\n",
        "        Split data and apply scaling\n",
        "        \"\"\"\n",
        "        print(\"\\n[4] Splitting Data (80/20 Stratified Split)...\")\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        "        )\n",
        "\n",
        "        print(f\"   Training set: {X_train.shape[0]} samples\")\n",
        "        print(f\"   Test set: {X_test.shape[0]} samples\")\n",
        "        print(f\"   Train class distribution:\\n{y_train.value_counts()}\")\n",
        "\n",
        "        # Scale features\n",
        "        print(\"\\n[5] Scaling Features...\")\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        # Convert back to DataFrame for easier handling\n",
        "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=self.feature_columns)\n",
        "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=self.feature_columns)\n",
        "\n",
        "        return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "\n",
        "    def handle_imbalance(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Apply SMOTE to handle class imbalance\n",
        "        \"\"\"\n",
        "        print(\"\\n[6] Handling Class Imbalance with SMOTE...\")\n",
        "\n",
        "        smote = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "        print(f\"   Original training samples: {len(y_train)}\")\n",
        "        print(f\"   After SMOTE: {len(y_train_balanced)}\")\n",
        "        print(f\"   Balanced class distribution:\\n{pd.Series(y_train_balanced).value_counts()}\")\n",
        "\n",
        "        return X_train_balanced, y_train_balanced"
      ],
      "metadata": {
        "id": "3SX7DzbEkIIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PHASE C: MODEL TRAINING & EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "class FraudDetectionModel:\n",
        "    \"\"\"\n",
        "    XGBoost-based fraud detection model with comprehensive evaluation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.feature_importance = None\n",
        "\n",
        "    def train_model(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Train XGBoost classifier optimized for precision-recall\n",
        "        \"\"\"\n",
        "        print(\"\\n[7] Training XGBoost Model...\")\n",
        "\n",
        "        # Calculate scale_pos_weight for imbalance (alternative to SMOTE)\n",
        "        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "        # XGBoost parameters optimized for fraud detection\n",
        "        params = {\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.1,\n",
        "            'n_estimators': 200,\n",
        "            'objective': 'binary:logistic',\n",
        "            'eval_metric': 'aucpr',  # Precision-Recall AUC\n",
        "            'scale_pos_weight': 1,  # Using SMOTE, so set to 1\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'random_state': RANDOM_STATE,\n",
        "            'tree_method': 'hist',\n",
        "            'enable_categorical': False\n",
        "        }\n",
        "\n",
        "        self.model = xgb.XGBClassifier(**params)\n",
        "\n",
        "        # Train model\n",
        "        self.model.fit(\n",
        "            X_train, y_train,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        print(\"   ‚úì Model training complete!\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def evaluate_model(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Comprehensive model evaluation with focus on precision-recall\n",
        "        \"\"\"\n",
        "        print(\"\\n[8] Evaluating Model Performance...\")\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        y_pred_proba = self.model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "        # Precision-Recall AUC\n",
        "        precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "        pr_auc = auc(recall_curve, precision_curve)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"MODEL EVALUATION METRICS\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Precision:           {precision:.4f}  (Minimize False Positives)\")\n",
        "        print(f\"Recall:              {recall:.4f}  (Catch High-Value Thefts)\")\n",
        "        print(f\"F1-Score:            {f1:.4f}  (Harmonic Mean)\")\n",
        "        print(f\"ROC-AUC:             {roc_auc:.4f}\")\n",
        "        print(f\"Precision-Recall AUC: {pr_auc:.4f}  ‚≠ê PRIMARY METRIC\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Detailed classification report\n",
        "        print(\"\\nDetailed Classification Report:\")\n",
        "        print(classification_report(y_test, y_pred,\n",
        "                                   target_names=['Normal (0)', 'Phishing (1)']))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(f\"                Predicted Normal  Predicted Phishing\")\n",
        "        print(f\"Actual Normal        {cm[0,0]:6d}         {cm[0,1]:6d}\")\n",
        "        print(f\"Actual Phishing      {cm[1,0]:6d}         {cm[1,1]:6d}\")\n",
        "\n",
        "        return {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'roc_auc': roc_auc,\n",
        "            'pr_auc': pr_auc,\n",
        "            'y_pred': y_pred,\n",
        "            'y_pred_proba': y_pred_proba\n",
        "        }\n",
        "\n",
        "    def plot_feature_importance(self, feature_names, top_n=15):\n",
        "        \"\"\"\n",
        "        Visualize feature importance to identify fraud indicators\n",
        "        \"\"\"\n",
        "        print(\"\\n[9] Analyzing Feature Importance...\")\n",
        "\n",
        "        # Get feature importance\n",
        "        importance = self.model.feature_importances_\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Importance': importance\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        print(\"\\nTop 10 Most Important Features:\")\n",
        "        for idx, row in feature_importance_df.head(10).iterrows():\n",
        "            print(f\"   {row['Feature']:30s}: {row['Importance']:.4f}\")\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        top_features = feature_importance_df.head(top_n)\n",
        "        plt.barh(range(len(top_features)), top_features['Importance'])\n",
        "        plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "        plt.xlabel('Importance Score', fontsize=12)\n",
        "        plt.ylabel('Feature', fontsize=12)\n",
        "        plt.title('Top Feature Importance for Fraud Detection', fontsize=14, fontweight='bold')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"\\n   ‚úì Feature importance plot saved as 'feature_importance.png'\")\n",
        "\n",
        "        return feature_importance_df"
      ],
      "metadata": {
        "id": "7XR6EpVdkL1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PHASE D: REAL-TIME FRAUD DETECTION & ALERTING\n",
        "# ============================================================================\n",
        "\n",
        "class SocialEngineeringDetector:\n",
        "    \"\"\"\n",
        "    Real-time detection system for address poisoning attacks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, preprocessor, feature_engineer):\n",
        "        self.model = model\n",
        "        self.preprocessor = preprocessor\n",
        "        self.feature_engineer = feature_engineer\n",
        "        self.alert_threshold = 0.7  # Probability threshold for alerts\n",
        "\n",
        "    def predict_transaction(self, transaction_data):\n",
        "        \"\"\"\n",
        "        Predict if a single transaction is fraudulent\n",
        "\n",
        "        Args:\n",
        "            transaction_data: Dict with keys matching original dataset columns\n",
        "        \"\"\"\n",
        "        # Convert to DataFrame\n",
        "        tx_df = pd.DataFrame([transaction_data])\n",
        "\n",
        "        # Apply same feature engineering\n",
        "        tx_df['TimeStamp'] = pd.to_datetime(tx_df['TimeStamp'], unit='s')\n",
        "        tx_df['is_dust'] = (tx_df['Value'] < 0.001).astype(int)\n",
        "        tx_df['value_log'] = np.log1p(tx_df['Value'])\n",
        "        tx_df['is_zero_value'] = (tx_df['Value'] == 0).astype(int)\n",
        "\n",
        "        # Apply stored frequency encodings\n",
        "        tx_df['from_address_freq'] = tx_df['From'].map(\n",
        "            self.feature_engineer.address_freq_from).fillna(1)\n",
        "        tx_df['to_address_freq'] = tx_df['To'].map(\n",
        "            self.feature_engineer.address_freq_to).fillna(1)\n",
        "\n",
        "        # Extract features that model expects\n",
        "        features = []\n",
        "        for col in self.preprocessor.feature_columns:\n",
        "            if col in tx_df.columns:\n",
        "                features.append(tx_df[col].values[0])\n",
        "            else:\n",
        "                features.append(0)  # Default value for missing features\n",
        "\n",
        "        # Scale features\n",
        "        features_scaled = self.preprocessor.scaler.transform([features])\n",
        "\n",
        "        # Predict\n",
        "        prediction = self.model.predict(features_scaled)[0]\n",
        "        probability = self.model.predict_proba(features_scaled)[0, 1]\n",
        "\n",
        "        return prediction, probability\n",
        "\n",
        "    def generate_alert(self, transaction_data, probability):\n",
        "        \"\"\"\n",
        "        Generate detailed security alert for suspicious transactions\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"üö®\" * 40)\n",
        "        print(\"‚ö†Ô∏è  SECURITY ALERT: POTENTIAL ADDRESS POISONING DETECTED\")\n",
        "        print(\"üö®\" * 40)\n",
        "        print(f\"\\nüìä Fraud Probability: {probability:.2%}\")\n",
        "        print(f\"\\nüìù Transaction Details:\")\n",
        "        print(f\"   Transaction Hash: {transaction_data.get('TxHash', 'N/A')}\")\n",
        "        print(f\"   From Address:     {transaction_data.get('From', 'N/A')}\")\n",
        "        print(f\"   To Address:       {transaction_data.get('To', 'N/A')}\")\n",
        "        print(f\"   Value:            {transaction_data.get('Value', 0)} ETH\")\n",
        "        print(f\"   Timestamp:        {datetime.fromtimestamp(transaction_data.get('TimeStamp', 0))}\")\n",
        "\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING INDICATORS:\")\n",
        "\n",
        "        if transaction_data.get('Value', 0) < 0.001:\n",
        "            print(f\"   üî∏ DUST TRANSACTION: Value ({transaction_data.get('Value', 0)} ETH) is extremely low\")\n",
        "            print(f\"      ‚Üí This is a common tactic in address poisoning attacks\")\n",
        "\n",
        "        print(f\"   üî∏ SUSPICIOUS PATTERN: Transaction matches known fraud signatures\")\n",
        "        print(f\"   üî∏ RECOMMENDATION: Verify recipient address carefully before sending funds\")\n",
        "\n",
        "        print(f\"\\nüõ°Ô∏è  PROTECTION ADVICE:\")\n",
        "        print(f\"   1. Always verify the FULL address, not just first/last characters\")\n",
        "        print(f\"   2. Use address book features instead of copying from transaction history\")\n",
        "        print(f\"   3. Consider this address as HIGH RISK\")\n",
        "        print(f\"   4. Double-check destination address on your hardware wallet\")\n",
        "\n",
        "        print(\"\\n\" + \"üö®\" * 40 + \"\\n\")\n",
        "\n",
        "    def monitor_transactions(self, transactions_df, sample_size=5):\n",
        "        \"\"\"\n",
        "        Demonstrate real-time monitoring on sample transactions\n",
        "        \"\"\"\n",
        "        print(\"\\n[10] Demonstrating Real-Time Transaction Monitoring...\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Sample random transactions\n",
        "        sample = transactions_df.sample(n=min(sample_size, len(transactions_df)))\n",
        "\n",
        "        alerts_triggered = 0\n",
        "\n",
        "        for idx, row in sample.iterrows():\n",
        "            tx_data = row.to_dict()\n",
        "            prediction, probability = self.predict_transaction(tx_data)\n",
        "\n",
        "            if probability >= self.alert_threshold:\n",
        "                self.generate_alert(tx_data, probability)\n",
        "                alerts_triggered += 1\n",
        "            else:\n",
        "                print(f\"‚úÖ Transaction {tx_data.get('TxHash', 'N/A')[:10]}... appears SAFE \"\n",
        "                      f\"(Fraud probability: {probability:.2%})\")\n",
        "\n",
        "        print(f\"\\nüìà Monitoring Summary: {alerts_triggered}/{sample_size} alerts triggered\")"
      ],
      "metadata": {
        "id": "ZyXPFHfxkQBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MAIN EXECUTION PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Execute complete fraud detection pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    CSV_FILE = '/content/1st dataset - balanced.csv'\n",
        "\n",
        "    try:\n",
        "        # Initialize components\n",
        "        feature_engineer = EthereumFeatureEngineering()\n",
        "        preprocessor = DataPreprocessor()\n",
        "        fraud_model = FraudDetectionModel()\n",
        "\n",
        "        # Load and engineer features\n",
        "        df = feature_engineer.load_and_analyze_data(CSV_FILE)\n",
        "        df_engineered = feature_engineer.engineer_features(df)\n",
        "\n",
        "        # Prepare features\n",
        "        X, y = preprocessor.prepare_features(df_engineered)\n",
        "        X_train, X_test, y_train, y_test = preprocessor.split_and_scale(X, y)\n",
        "\n",
        "        # Handle imbalance\n",
        "        X_train_balanced, y_train_balanced = preprocessor.handle_imbalance(X_train, y_train)\n",
        "\n",
        "        # Train model\n",
        "        model = fraud_model.train_model(X_train_balanced, y_train_balanced)\n",
        "\n",
        "        # Evaluate\n",
        "        metrics = fraud_model.evaluate_model(X_test, y_test)\n",
        "\n",
        "        # Feature importance\n",
        "        feature_importance = fraud_model.plot_feature_importance(preprocessor.feature_columns)\n",
        "\n",
        "        # Real-time detection demo\n",
        "        detector = SocialEngineeringDetector(model, preprocessor, feature_engineer)\n",
        "        detector.monitor_transactions(df_engineered, sample_size=3)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"‚úÖ PIPELINE EXECUTION COMPLETE\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"\\nüìä Final Model Performance Summary:\")\n",
        "        print(f\"   ‚Ä¢ Precision-Recall AUC: {metrics['pr_auc']:.4f}\")\n",
        "        print(f\"   ‚Ä¢ F1-Score: {metrics['f1']:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Precision: {metrics['precision']:.4f}\")\n",
        "        print(f\"   ‚Ä¢ Recall: {metrics['recall']:.4f}\")\n",
        "        print(f\"\\nüíæ Model and visualizations saved successfully!\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"\\n‚ùå ERROR: Could not find '{CSV_FILE}'\")\n",
        "        print(\"Please ensure the CSV file is in the same directory as this script.\")\n",
        "        print(\"Or update the CSV_FILE variable with the correct path.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1o8Z82yjHG2",
        "outputId": "40ba236f-7f09-4f40-9af8-acd313fe3131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1] Loading Dataset...\n",
            "Dataset Shape: (95300, 9)\n",
            "\n",
            "Column Names: ['TxHash', 'BlockHeight', ' TimeStamp', 'From', 'To', 'Value', 'ContractAddress', 'Input', 'Class']\n",
            "\n",
            "Data Types:\n",
            "TxHash              object\n",
            "BlockHeight          int64\n",
            " TimeStamp           int64\n",
            "From                object\n",
            "To                  object\n",
            "Value              float64\n",
            "ContractAddress     object\n",
            "Input               object\n",
            "Class              float64\n",
            "dtype: object\n",
            "\n",
            "Missing Values:\n",
            "TxHash                 0\n",
            "BlockHeight            0\n",
            " TimeStamp             0\n",
            "From                   0\n",
            "To                   110\n",
            "Value                  0\n",
            "ContractAddress    95190\n",
            "Input               3643\n",
            "Class                  1\n",
            "dtype: int64\n",
            "\n",
            "Class Distribution:\n",
            "Class\n",
            "0.0    79216\n",
            "1.0    16083\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class Percentage:\n",
            "Class\n",
            "0.0    83.123642\n",
            "1.0    16.876358\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "[2] Engineering Behavioral Features...\n",
            "   ‚Üí Cleaning data...\n",
            "\n",
            "‚ùå ERROR: 'TimeStamp'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'TimeStamp'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3701241975.py\", line 20, in main\n",
            "    df_engineered = feature_engineer.engineer_features(df)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2617071249.py\", line 49, in engineer_features\n",
            "    df['TimeStamp'] = pd.to_datetime(df['TimeStamp'], unit='s')\n",
            "                                     ~~^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\", line 4102, in __getitem__\n",
            "    indexer = self.columns.get_loc(key)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'TimeStamp'\n"
          ]
        }
      ]
    }
  ]
}